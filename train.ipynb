{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GEqrisqm0veOrdQCCHSxnbxw4yKi4ooS",
      "authorship_tag": "ABX9TyNajomgmNwtbXHDik/g1LRe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/handsomecoderyang/deep-learning-for-image-processing/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX\n",
        "# pip install utils\n",
        "# from utils.vis_tools import Visualizer\n",
        " "
      ],
      "metadata": {
        "id": "aR3u8Pj_G3va",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "f5c1c152-775e-41b7-d112-ce39c7dcf950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-4a8cdc8d0122>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install tensorboardX\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pathlib\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SliceData(Dataset):\n",
        "  def __init__(self, root, transform):\n",
        "    self.transform = transform\n",
        "    self.examples = []\n",
        "    files = list(pathlib.Path(root).iterdir())\n",
        "    for fname in files:\n",
        "      kspace = np.load(fname)\n",
        "      num_slices = kspace.shape[0]\n",
        "      self.examples += [(fname, slice) for slice in range(num_slices)] #文件+切片标号\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    fname, slice_num = self.examples[index]\n",
        "    data = np.load(fname)\n",
        "    data_kspace = data[slice_num]\n",
        "    data_kspace = torch.from_numpy(data_kspace)\n",
        "    # target_kspace = torch.complex(data_kspace[:, :, 0], data_kspace[:, :, 1])\n",
        "    return self.transform(data_kspace, fname=fname, slice='slice_num')\n",
        "\n"
      ],
      "metadata": {
        "id": "Oe2gvk6pj2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.activation import LeakyReLU\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, inchans:int, outchans: int, drop_prob: float):\n",
        "    super().__init__()\n",
        "\n",
        "    self.inchans = inchans\n",
        "    self.outchans = outchans\n",
        "    self.drop_prob = drop_prob\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(self.inchans, self.outchans, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(self.outchans),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "        nn.Dropout2d(drop_prob),\n",
        "\n",
        "        nn.Conv2d(self.outchans, self.outchans, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(self.outchans),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "        nn.Dropout2d(drop_prob),\n",
        "\n",
        "    )\n",
        "  def forward(self, image: torch.tensor):\n",
        "    return self.layers(image)\n",
        "\n",
        "class TransposeConvBlock(nn.Module):\n",
        "  def __init__(self, inchans: int, outchans: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.inchans = inchans\n",
        "    self.outchans = outchans\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.ConvTranspose2d(self.inchans, self.outchans, kernel_size=2, stride=2, bias=False),\n",
        "        nn.InstanceNorm2d(self.outchans),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "    )\n",
        "  def forward(self, image: torch.tensor):\n",
        "    return self.layers(image)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "  #Une框架参考原始论文\n",
        "\n",
        "  def __init__(self, in_chans: int, out_chans: int, chans: int = 32,\n",
        "               num_pool_layers: int = 4, drop_prob: float = 0.0): #256 * 256\n",
        "    super().__init__()\n",
        "\n",
        "    self.inchans = in_chans\n",
        "    self.out_chans = out_chans\n",
        "    self.chans = chans\n",
        "    self.num_pool_layers = num_pool_layers\n",
        "    self.drop_prob = drop_prob\n",
        "    self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
        "    ch = chans\n",
        "    for _ in range(num_pool_layers -1):\n",
        "      self.down_sample_layers.append(ConvBlock(ch, ch*2, drop_prob))\n",
        "      ch *= 2\n",
        "    self.conv = ConvBlock(ch, ch * 2, drop_prob)  \n",
        "\n",
        "    self.up_conv = nn.ModuleList()\n",
        "    self.up_transpose_conv = nn.ModuleList()\n",
        "    for _ in range(num_pool_layers - 1):\n",
        "      self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
        "      self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n",
        "      ch //= 2\n",
        "    self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n",
        "    self.up_conv.append(\n",
        "        nn.Sequential(\n",
        "            ConvBlock(ch * 2, ch, drop_prob),\n",
        "            nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1)\n",
        "        )\n",
        "    )\n",
        "\n",
        "  def forward(self, image: torch.Tensor):\n",
        "    #首先进行下采样\n",
        "    stack = []  #保存每一层的输出结果\n",
        "    output = image #表示输入\n",
        "    for layer in self.down_sample_layers:\n",
        "      print(layer)\n",
        "      output = layer(output)\n",
        "      stack.append(output)\n",
        "      F.avg_pool2d(output, kernel_size=2, stride=2) #总共4层， 每层下降2倍， 总共下降16倍， 256/16 = 16, 所以此时output 大小为[batchsize, 512, 16, 16]\n",
        "    self.conv(output)\n",
        "\n",
        "    #接着进行上采样(转置卷积)\n",
        "    for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n",
        "\n",
        "      print(transpose_conv, '\\n', conv)\n",
        "      downsample_layer = stack.pop()\n",
        "      output = transpose_conv(output)\n",
        "\n",
        "      #做pad保证输出的和pop出的数据的大小相同 [N, chanel, H, W]\n",
        "      padding = [0, 0, 0, 0] #左右上下\n",
        "      if output.shape[-1] != downsample_layer.shape[-1]:\n",
        "        padding[1] = 1\n",
        "      if output.shape[-2] != downsample_layer.shape[-2]:\n",
        "        padding[3] = 1\n",
        "      if torch.sum(torch.tensor(padding)) != 0:\n",
        "        output = F.pad(output, padding, \"reflect\")\n",
        "      \n",
        "      output = torch.cat((downsample_layer, output), dim=1)\n",
        "      output = conv(output)             #[batchsize, 1, 256, 256]\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "JOsdsaSka0KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkaHp-qtFQne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee00ee4c-9975-4de7-8dff-262d1e4fa286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pathlib.Path'>\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import pathlib\n",
        "import shutil\n",
        "import time\n",
        "import numpy as numpy\n",
        "import torch\n",
        "import torchvision\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import scipy.io as sio\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# sys.path.remove(\"/content/drive/MyDrive/MRI\")\n",
        "# print(sys.path)\n",
        "os.environ[\"CUDA_VISBLE_DIVICES\"] = \"0\"\n",
        "# import SliceData\n",
        "# from utils.vis_tools import Visualizer\n",
        "# from unet import Unet\n",
        "print(pathlib.Path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig()\n",
        "logger = logging.getLogger(__name__)\n",
        "\n"
      ],
      "metadata": {
        "id": "FdhTOG8wburg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataTransform:\n",
        "  def __init__(self, mask):\n",
        "    self.mask = torch.from_numpy(mask)\n",
        "\n",
        "  def __call__(self, kspace, fname, slice):\n",
        "    target_kspace = kspace\n",
        "    target_kspace = torch.complex(target_kspace[:, :, 0], target_kspace[:, :, 1])\n",
        "    under_kspace = torch.mul(target_kspace, self.mask)\n",
        "\n",
        "    target_img = torch.abs(torch.fft.ifft2(target_kspace))\n",
        "    under_img = torch.abs(torch.fft.ifft2(under_kspace))\n",
        "\n",
        "    return target_img, under_img"
      ],
      "metadata": {
        "id": "zzf7baUEeV5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datasets(args):\n",
        "  mask = sio.loadmat(\"./mask/%s/%s/%s_256_256_%d.mat\" % (args.data, args.mask, args.mask, args.rate))['Umask']\n",
        "  train_data = SliceData(\n",
        "      root=args.data_path/f'Train_part1',\n",
        "      transform=DataTransform(mask)\n",
        "  )\n",
        "  dev_data = SliceData(\n",
        "      root=args.data_path/f'Val',\n",
        "      transform=DataTransform(mask)\n",
        "  )\n",
        "  return dev_data, train_data"
      ],
      "metadata": {
        "id": "1RgePsPFekZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders(args):\n",
        "  dev_data, train_data = create_datasets(args)\n",
        "  display_data = [dev_data[i] for i in range(0, len(dev_data), len(dev_data))]\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      dataset=train_data,\n",
        "      batch_size = args.batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=4,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  dev_loader = DataLoader(\n",
        "      dataset=dev_data,\n",
        "      batch_size = args.batch_size,\n",
        "      num_workers = 4,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  display_loader = DataLoader(\n",
        "      dataset=display_data,\n",
        "      batch_size = 16,\n",
        "      num_workers = 4,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_loader, dev_loader, display_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "6Xa7_CMXIVV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(args, epoch, model, data_loader, optimizer, writer):\n",
        "  model.train()\n",
        "  avg_loss = 0\n",
        "  start_epoch = start_iter = time.perf_counter() #记录当前epoch开始的时间\n",
        "  global_step = epoch * len(data_loader)\n",
        "  for iter, data in enumerate(data_loader):\n",
        "    under_img_tensor, target_img_tensor = data\n",
        "    under_img_tensor, target_img_tensor = under_img_tensor.float(), target_img_tensor.float()\n",
        "\n",
        "    under_img_tensor = under_img_tensor.unsqueeze(1).to(device)\n",
        "    \n",
        "    print(under_img_tensor.size(), target_img_tensor.size())\n",
        "\n",
        "    output = model(under_img_tensor).squeeze(1)\n",
        "    loss = F.l1_loss(output, target_img_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
        "    writer.add_scalar('TrainLoss', loss.item(), global_step + iter)\n",
        "\n",
        "    if iter % args.report_interval == 0:\n",
        "      logging.info(f'Epoch = [{epoch:3d}/{args.epochs:3d}]',\n",
        "            f'Iter = [{iter:4d}/{len(data_loader):4d}]',\n",
        "            f'Loss = {loss.item():.4g} Avg_loss = {avg_loss:.4g}'\n",
        "            f'Time = {time.per_counter() - start_iter:.4f}s',\n",
        "      )\n",
        "      start_iter = time.perf_counter()\n",
        "  return avg_loss, time.perf_counter() - start_epoch"
      ],
      "metadata": {
        "id": "qL4cug1FBW5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(args, epoch: int, model, data_loader, writer, vis):\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  start = time.per_counter()\n",
        "  with torch.no_grad():\n",
        "    for iter, data in enumerate(data_loader):\n",
        "      under_img_tensor, target_img_tensor = data\n",
        "      under_img_tensor = under_img_tensor.unsqueeze(1).to(device)\n",
        "      target_img_tensor = target_img_tensor.to(device)\n",
        "\n",
        "      output = model(under_img_tensor).squeeze(1)\n",
        "      loss = F.mse_loss(output, target_img_tensor)\n",
        "      losses.append(loss.item())\n",
        "    writer.add_scalar('Dev_loss', np.mean(losses), epoch)\n",
        "    if (vis != None):\n",
        "      vis.plot(\"val Loss\", np.mean(losses))\n",
        "\n",
        "  return np.mean(losses), time.perf_counter() - start\n"
      ],
      "metadata": {
        "id": "k7GxSJT8GWd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(args, epoch, model, data_loader, writer, vis):\n",
        "  def save_image(image, tag):\n",
        "    image -= image.min()\n",
        "    image /= image.max()\n",
        "    grid = torchvision.utils.make_grid(image, nrow=4, pad_value=1)\n",
        "    writer.add_image(tag, grid, epoch)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for iter, data in enumerate(data_loader):\n",
        "      under_img_tensor, target_img_tensor = data\n",
        "      under_img_tensor = under_img_tensor.unsqueeze(1).to(device)\n",
        "      target_img_tensor = target_img_tensor.to(device)\n",
        "      output = model(under_img_tensor)\n",
        "\n",
        "      if(vis != None):\n",
        "        for i in range(len(output)):\n",
        "          vis.img(\"undersampled image - %d\"%(i), under_img_tensor.squeeze(1)[i])\n",
        "          vis.img(\"full image - %d\"%(i), target_img_tensor[i])\n",
        "          vis.img(\"recons image - %d\"%(i), output[i])\n",
        "        save_image(target_img_tensor.unsqueeze(1), 'Target')\n",
        "        save_image(output, 'Reconstruction')\n",
        "        save_image(torch.abs(target_img_tensor.unsqueeze(1) - output), 'Error')\n",
        "      break"
      ],
      "metadata": {
        "id": "Naau47esImV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(args, exp_dir, epoch, model, optimizer, best_dev_loss, is_new_best):\n",
        "  torch.save(\n",
        "      {\n",
        "          'epoch': epoch,\n",
        "          'args': args,\n",
        "          'model': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "          'best_dev_loss': best_dev_loss,\n",
        "          'exp_dir': exp_dir\n",
        "      },\n",
        "      f = exp_dir / 'model.pt'\n",
        "  )\n",
        "  if is_new_best:\n",
        "    shutil.copyfile(exp_dir / 'model.pt', exp_dir / 'best_model.pt')\n"
      ],
      "metadata": {
        "id": "U1XkxGMsRfIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(args):\n",
        "  model = Unet(1, 1, 64, 4, 0).to(device)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "OUTxLWD9S-9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_model(checkpoint_files):\n",
        "  checkpoint = torch.load(checkpoint_files)\n",
        "  args = checkpoint['args']\n",
        "  model = build_model(args)\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  optimizer = build_optim(args, model.parameters())\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  return checkpoint, model, optimizer"
      ],
      "metadata": {
        "id": "vOJKoR82TUmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optim(args, params):\n",
        "  optimizer = torch.optim.Adam(params, args.lr)\n",
        "  return optimizer\n",
        "  "
      ],
      "metadata": {
        "id": "UnpUUtiqUY0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args, vis):\n",
        "  args.exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "  writer = SummaryWriter(log_dir=args.exp_dir / 'summary')\n",
        "\n",
        "  if args.resume:\n",
        "    checkpoint, model, optimizer = load_model(\"./checkpoint\")  #************************************************************************\n",
        "    args = checkpoint['args']\n",
        "    best_dev_loss = checkpoint['best_dev_loss']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    del checkpoint\n",
        "  else:\n",
        "    model = build_model(args)\n",
        "    optimizer = build_optim(args, model.parameters())\n",
        "    best_dev_loss = 1e9\n",
        "    start_epoch = 0\n",
        "  logging.info(args)\n",
        "  logging.info(model)\n",
        "\n",
        "  train_loader, dev_loader, display_loader = create_data_loaders(args)\n",
        "\n",
        "  for epoch in range(start_epoch, args.epochs):\n",
        "    train_loss, train_time = train_epoch(args, epoch, model, train_loader, optimizer, writer)\n",
        "\n",
        "    dev_loss, dev_time = evaluate(args, epoch, model, dev_loader, writer, vis)\n",
        "    visualize(args, epoch, model, display_loader, writer, vis)\n",
        "\n",
        "    is_new_best = dev_loss < best_dev_loss\n",
        "    save_model(args, args.exp_dir, epoch, model, optimizer, best_dev_loss, is_new_best)\n",
        "    logging.info(\n",
        "        f'epoch = [{epoch:4d}/{args.epochs:4d}] TrainLoss = {train_loss:.4g}'\n",
        "        f'DevLoss = {dev_loss:.4g} TrainTime = {train_time:.4f}s DevTime = {dev_time:.4f}s',\n",
        "    )\n",
        "    writer.close() "
      ],
      "metadata": {
        "id": "WPeIZqkMU6K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "def create_arg_parser():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')\n",
        "  parser.add_argument('--batch_size', default=1, type=int, help='the batch size')\n",
        "  parser.add_argument('--lr', default=0.0005, type=float, help='the learning rate')\n",
        "  parser.add_argument('--rate', default=20, type=int, choices=[5, 10, 20, 25], help='the undersampling rate')\n",
        "  parser.add_argument('--mask', default='radial', type=str, choices=['catesian', 'radial', 'random'], help='the type of mask')\n",
        "  parser.add_argument('--data', default='brain', type=str, choices=['brain', 'kenn'], help='which dataset(brain or knee')\n",
        "  parser.add_argument('--report_interval', default=100, type=int, help='period of loss reporting')\n",
        "  parser.add_argument('--exp_dir', default='cheakpoints', type=pathlib.Path, help='path where model and results should be saved')\n",
        "  parser.add_argument('--resume', action='store_true', help='if set, resume the training from a previous model checkpoint.''\"--cheakpoint\" should be set with this')\n",
        "  parser.add_argument('--checkpoint', default=\"/content/drive/MyDrive/MRI/\", type=str, help='path to an existing checkpoint. used along with \"--resume')\n",
        "  parser.add_argument('--data_path', default=\"/content/drive/MyDrive/MRI/\", type=pathlib.Path, required=False, help='path to the dataset')\n",
        "  parser.add_argument('--device', type=str, default='cuda:0', help='which device to train on.set to \"cuda:n\", n represent GPU number')\n",
        "  parser.add_argument('--use_visdom', type=bool, default=False, help='if true, watch loss and reconstruction on port http://localhost:8097')\n",
        "  return parser"
      ],
      "metadata": {
        "id": "nBEu2nDUNF9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\"\n",
        "root_mri = '/content/drive/MyDrive/MRI/'\n",
        "os.chdir(root_mri)\n",
        "# !pwd\n",
        "# !ls\n",
        "vis = None\n",
        "# args = create_arg_parser().parse_args()\n",
        "args = create_arg_parser().parse_known_args()[0]\n",
        "print(args)\n",
        "main(args, vis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5kTNIGhLdOpf",
        "outputId": "70a8d250-9d69-4e78-ae2b-e597daaa43ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=1, checkpoint='/content/drive/MyDrive/MRI/', data='brain', data_path=PosixPath('/content/drive/MyDrive/MRI'), device='cuda:0', epochs=50, exp_dir=PosixPath('cheakpoints'), lr=0.0005, mask='radial', rate=20, report_interval=100, resume=False, use_visdom=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 256, 256]) torch.Size([1, 256, 256])\n",
            "ConvBlock(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (3): Dropout2d(p=0, inplace=False)\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (7): Dropout2d(p=0, inplace=False)\n",
            "  )\n",
            ")\n",
            "ConvBlock(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (3): Dropout2d(p=0, inplace=False)\n",
            "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (7): Dropout2d(p=0, inplace=False)\n",
            "  )\n",
            ")\n",
            "ConvBlock(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (3): Dropout2d(p=0, inplace=False)\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (7): Dropout2d(p=0, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1b9644b07043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_arg_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-74af65f7ef56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, vis)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-94f98fdc9021>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, epoch, model, data_loader, optimizer, writer)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder_img_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_img_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder_img_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_img_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2b0703f65514>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_sample_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#总共4层， 每层下降2倍， 总共下降16倍， 256/16 = 16, 所以此时output 大小为[batchsize, 512, 16, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2b0703f65514>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTransposeConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m         return F.instance_norm(\n\u001b[1;32m     58\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         \u001b[0m_verify_spatial_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m     return torch.instance_norm(\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m     )\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.17 GiB total capacity; 10.61 GiB already allocated; 38.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install psmisc\n",
        "# !sudo fuser /dev/nvidia*\n",
        "# !kill -9 73\n"
      ],
      "metadata": {
        "id": "JW_6I-gRFIdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}